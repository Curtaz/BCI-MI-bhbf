random_seed: 42 # Set it to None to implement fully random behaviour #TODO IMPLEMENT THIS COME ON!

hydra:
  run:
    dir: ${log_path}

## Data Processing options
data_path: "C:/Users/tomma/Documents/Uni/PhD/data/cBCI/sbj4_tr_rigorous/preprocessed"
output_path: "C:/Users/tomma/Documents/Uni/PhD/code/BCI-MI-bhbf/model/sbj4_tr_rigorous_${now:%Y%m%d_%H%M%S}"
log_path: "C:/Users/tomma/Documents/Uni/PhD/code/BCI-MI-bhbf/logs/sbj4_tr_rigorous_${now:%Y%m%d_%H%M%S}"
sample_frequency: 512
win_size_s: 1
win_shift_s: 0.0625
num_channels: 32
label_map:
  780: 0 # BOTH_HANDS
  781: 1 # BOTH_FEET
label_names:
  0: "Both Hands" 
  1: "Both Feets" # Optional mapping (after dropping unwanted labels)
normalize: "zscore" # Data normalization applied at the end of the processing. Available options are "zscore", None

## Model options
TGCN:
  # TGCN Params
  gcn_hidden_dims: [128]
  gcn_output_dim: 16
  gcn_activation: F.leaky_relu
  gru_hidden_units: 128
  gcn_dropout: 0.5
  gru_dropout: 0.3

EEGNET:
  # eegnet params
  conv_dropout: 0.5
  f1: 8
  d: 2
  f2: 16
  embedding_dim: 32

## training options
train:
  metric: cosine # one of 'cosine', 'euclidean'
  lr: 0.01
  es_patience: 5
  es_min_delta: 0.0001

  num_epochs: 50
  eval_batch_size: 128

  n_support: 50
  n_query: 60
  n_episodes: 500
  max_grad_norm: 1
  num_classes: 2

  log_interval: 1

  optimizer: 
    class: Adam
    options: 
      lr: 0.01
    
  scheduler:
    class: LinearLR
    options: 
      start_factor: 1
      end_factor: 0.0001
      total_iters: ${num_epochs}

  use_wandb: false
  
  device: "cuda"